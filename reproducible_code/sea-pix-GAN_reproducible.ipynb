{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoSw1xXI4Ogl"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from scipy import ndimage, stats\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset, TensorDataset\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the device\n",
        "if th.cuda.is_available():\n",
        "    device = th.device(\"cuda\")\n",
        "    print(f\"Using device {device}: {th.cuda.get_device_name()}\")\n",
        "elif th.backends.mps.is_available():\n",
        "    device = th.device(\"mps\")\n",
        "    print(f\"Using device {device}\")\n",
        "else:\n",
        "    device = th.device(\"cpu\")\n",
        "    print(f\"Using device {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcYIC5TQ4Ogx"
      },
      "outputs": [],
      "source": [
        "# Manually override to cpu\n",
        "# device = th.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvjcpCXO4Ogz"
      },
      "source": [
        "# Hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybKr4PD54Og3"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 150,\n",
        "    \"l1_lambda\": 100,\n",
        "    \"adam_betas\": (0.5, 0.999), # default values are (0.9, 0.999)\n",
        "    \"data_splits\": [4/6, 1/6, 1/6],\n",
        "    \"test_run\": False, # Set to True to run a test with a smaller dataset\n",
        "    \"test_run_size\": 192,\n",
        "}\n",
        "\n",
        "# Create the path and model data directory itself\n",
        "model_data = \"./rsc/first_model\"\n",
        "if not os.path.exists(model_data):\n",
        "    os.makedirs(model_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Vun8yv4Og4"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUz-flZc4Og5"
      },
      "outputs": [],
      "source": [
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, input_data_dir: str, gt_data_dir: str, size=None, excluded=None, random_perturbations=True):\n",
        "        # Define the data directories and the list of image names (same for input and gt)\n",
        "        self.input_data_dir = input_data_dir\n",
        "        self.gt_data_dir = gt_data_dir\n",
        "        self.image_files = os.listdir(input_data_dir)\n",
        "\n",
        "        # Select the correct amount of images if a size is provided (for smaller experiments)\n",
        "        if size is not None:\n",
        "            self.image_files = self.image_files[:size]\n",
        "\n",
        "        # Filter out any image names if provided (to keep figure images out of training)\n",
        "        if excluded is not None:\n",
        "            self.image_files = [filename for filename in self.image_files if filename not in excluded]\n",
        "\n",
        "        # Define the image transform\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize([256, 256], interpolation=T.InterpolationMode.NEAREST),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomVerticalFlip(),\n",
        "            T.RandomCrop(256),\n",
        "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ]) if random_perturbations else T.Compose([\n",
        "            T.Resize([256, 256], interpolation=T.InterpolationMode.NEAREST),\n",
        "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of image pairs in the dataset\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Get the image paths for both the input and the gt\n",
        "        input_image_path = os.path.join(self.input_data_dir, self.image_files[i])\n",
        "        gt_image_path = os.path.join(self.gt_data_dir, self.image_files[i])\n",
        "\n",
        "        # Read in the input image and correct the color and channel order (cv2 reads in BGR)\n",
        "        input_image = cv2.imread(input_image_path)\n",
        "        input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Read in the gt image and correct the color and channel order (cv2 reads in BGR)\n",
        "        gt_image = cv2.imread(gt_image_path)\n",
        "        gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert numpy arrays to torch tensors via PIL images\n",
        "        input_image = T.ToTensor()(Image.fromarray(input_image))\n",
        "        gt_image = T.ToTensor()(Image.fromarray(gt_image))\n",
        "\n",
        "        # Stack images along the batch dimension and transform them together (for equal augmentation)\n",
        "        stacked_images = th.stack([input_image, gt_image])\n",
        "        transformed_images = self.transform(stacked_images)\n",
        "\n",
        "        # Return the transformed images ([input_image, gt_image])\n",
        "        return transformed_images[0], transformed_images[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvgdHm8Y4Og6"
      },
      "source": [
        "## Running dataset construction code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc6y3fw44Og8"
      },
      "outputs": [],
      "source": [
        "# Get the dataset paths\n",
        "figure_input_data_dir = \"./FigureImages/input\"\n",
        "figure_gt_data_dir = \"./FigureImages/ground_truth\"\n",
        "input_data_dir = \"./UnderwaterImages/input\"\n",
        "gt_data_dir = \"./UnderwaterImages/ground_truth\"\n",
        "#input_data_dir = \"./Paired/underwater_scenes/trainA\"\n",
        "#gt_data_dir = \"./Paired/underwater_scenes/trainB\"\n",
        "\n",
        "# Create the main (split up) and figure dataset\n",
        "excluded = os.listdir(figure_input_data_dir)\n",
        "if hyperparams[\"test_run\"]:\n",
        "    dataset = PairedImageDataset(\n",
        "                                input_data_dir,\n",
        "                                gt_data_dir,\n",
        "                                size=hyperparams[\"test_run_size\"],\n",
        "                                excluded=excluded\n",
        "                                )\n",
        "else:\n",
        "    dataset = PairedImageDataset(\n",
        "                                input_data_dir,\n",
        "                                gt_data_dir,\n",
        "                                excluded=excluded\n",
        "                                )\n",
        "train_set, validation_set, test_set = random_split(dataset, hyperparams[\"data_splits\"])\n",
        "figure_dataset = PairedImageDataset(\n",
        "                                    figure_input_data_dir,\n",
        "                                    figure_gt_data_dir,\n",
        "                                    random_perturbations=False\n",
        "                                    )\n",
        "\n",
        "# Create the dataloaders\n",
        "train_set = DataLoader(dataset=train_set, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
        "validation_set = DataLoader(dataset=validation_set, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
        "test_set = DataLoader(dataset=test_set, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
        "figure_set = DataLoader(dataset=figure_dataset, batch_size=len(figure_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJQcrSTf4Og9"
      },
      "source": [
        "# Models\n",
        "\n",
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXt2rQWv4Og-"
      },
      "outputs": [],
      "source": [
        "class DownModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, leaky_relu_slope=0.2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.lrelu = nn.LeakyReLU(leaky_relu_slope)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.lrelu(x)\n",
        "        return x\n",
        "\n",
        "class ZeroPadModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, DEBUG=False):\n",
        "        super().__init__()\n",
        "        self.DEBUG = DEBUG\n",
        "\n",
        "        self.DownLayers = nn.Sequential(\n",
        "            DownModule(6, 64),\n",
        "            DownModule(64, 128),\n",
        "            DownModule(128, 256),\n",
        "            ZeroPadModule(),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            ZeroPadModule(),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
        "            nn.Sigmoid() #NOTE: Not actually in the paper, but required for the BCELoss (limits values to [0,1]). This produces a valid probability tensor.\n",
        "        )\n",
        "\n",
        "    def forward(self, x: th.Tensor, y: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"Forward pass of the discriminator\n",
        "\n",
        "        Args:\n",
        "            x (th.Tensor): Raw underwater image\n",
        "            y (th.Tensor): Enhanced underwater image\n",
        "\n",
        "        Returns:\n",
        "            th.Tensor: Output tensor measuring the realness of the input images\n",
        "        \"\"\"\n",
        "\n",
        "        z = th.concatenate((x, y), dim=1)\n",
        "\n",
        "        # Input tensor shape\n",
        "        if self.DEBUG:\n",
        "            print(\"Input tensor shape:\")\n",
        "            print(z.shape)\n",
        "\n",
        "        for layer in self.DownLayers:\n",
        "            z = layer(z)\n",
        "            if self.DEBUG:\n",
        "                print(z.shape)\n",
        "\n",
        "        return z\n",
        "\n",
        "#discriminator = Discriminator(DEBUG=True).to(device)\n",
        "\n",
        "#sample = th.rand(1, 3, 256, 256, device=device)\n",
        "#clone = sample.clone()\n",
        "#output = discriminator(sample, clone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CVTvQsJ4Og_"
      },
      "source": [
        "## Generator / Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYXZbOjj4OhA"
      },
      "outputs": [],
      "source": [
        "class EncoderModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, leaky_relu_slope=0.2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.lrelu = nn.LeakyReLU(leaky_relu_slope)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.lrelu(x)\n",
        "        return x\n",
        "\n",
        "class FeatureMapModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, leaky_relu_slope=0.2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.lrelu = nn.LeakyReLU(leaky_relu_slope)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.lrelu(x)\n",
        "        return x\n",
        "\n",
        "class DecoderModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.deconv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class OutputModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder model for image generation\n",
        "\n",
        "    A residual autoencoder model for image generation.\n",
        "    The final model will be an image-to-image translation model\n",
        "    that enhances underwater images.\n",
        "    \"\"\"\n",
        "    def __init__(self, DEBUG=False):\n",
        "        super().__init__()\n",
        "        self.DEBUG = DEBUG\n",
        "\n",
        "        self.EncoderLayers = nn.ModuleList([\n",
        "            EncoderModule(3, 64),\n",
        "            EncoderModule(64, 128),\n",
        "            EncoderModule(128, 256),\n",
        "            EncoderModule(256, 512),\n",
        "            EncoderModule(512, 512),\n",
        "            EncoderModule(512, 512),\n",
        "            EncoderModule(512, 512),\n",
        "            FeatureMapModule(512, 512),\n",
        "        ])\n",
        "\n",
        "        self.DecoderLayers = nn.ModuleList([\n",
        "            DecoderModule(512, 512),\n",
        "            DecoderModule(1024, 512),\n",
        "            DecoderModule(1024, 512),\n",
        "            DecoderModule(1024, 512, dropout_prob=0.0),\n",
        "            DecoderModule(1024, 256, dropout_prob=0.0),\n",
        "            DecoderModule(512, 128, dropout_prob=0.0),\n",
        "            DecoderModule(256, 64, dropout_prob=0.0),\n",
        "        ])\n",
        "\n",
        "        self.OutputLayer = OutputModule(128, 3)\n",
        "        self.tanh = nn.Tanh() #NOTE: Not actually in the paper, but required to limit values to [0,1]. This produces a valid (float) image tensor.\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass for the autoencoder model.\n",
        "\n",
        "        Args:\n",
        "            x (th.Tensor): Input image tensor\n",
        "\n",
        "        Returns:\n",
        "            th.Tensor: Output image tensor\n",
        "        \"\"\"\n",
        "        # Store the activations of the encoder layers for skip connections\n",
        "        layer_outputs = []\n",
        "\n",
        "        if self.DEBUG:\n",
        "            print(\"Starting forward pass\")\n",
        "            print(x.shape)\n",
        "\n",
        "        # Encoder pass\n",
        "        for i in range(len(self.EncoderLayers)):\n",
        "            x = self.EncoderLayers[i](x)\n",
        "            if i < len(self.EncoderLayers) - 1:\n",
        "                layer_outputs.append(x)\n",
        "            if self.DEBUG:\n",
        "                print(x.shape)\n",
        "\n",
        "        if self.DEBUG:\n",
        "            print(\"Encoding complete\")\n",
        "            print(x.shape)\n",
        "\n",
        "        # Checking the shapes of the stored activations\n",
        "        #[print(\"Stored activations: \",x.shape) for x in layer_outputs]\n",
        "\n",
        "        # Decoder pass\n",
        "        for i in range(len(self.DecoderLayers)):\n",
        "\n",
        "            if i != 0:\n",
        "                # Get the appropriate encoder activation\n",
        "                s = layer_outputs.pop()\n",
        "\n",
        "                # If the shapes match, concatenate the activations\n",
        "                if x.shape == s.shape:\n",
        "                    x = th.cat((x, s), 1)\n",
        "\n",
        "                else:\n",
        "                    print(\"Error, shapes do not match\")\n",
        "                    print(\"X:\", x.shape)\n",
        "                    print(\"S:\", s.shape)\n",
        "                    return th.tensor([])\n",
        "\n",
        "            # Pass the concatenated activations through the decoder layer\n",
        "            x = self.DecoderLayers[i](x)\n",
        "            if self.DEBUG:\n",
        "                print(x.shape)\n",
        "\n",
        "        if self.DEBUG:\n",
        "            print(\"Decoding complete\")\n",
        "\n",
        "        # Perform the final deconvolution\n",
        "        x = th.cat((x, layer_outputs.pop()), 1)\n",
        "        x = self.OutputLayer(x)\n",
        "        x = self.tanh(x)\n",
        "\n",
        "        if self.DEBUG:\n",
        "            print(\"Is layer_outputs empty:\", len(layer_outputs) == 0)\n",
        "            print(x.shape)\n",
        "            print(\"Output complete\")\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayq5ebWd4OhB"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "## Defining the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQwZkC5A4OhB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train_loop(dataloader, discriminator, generator, d_optimizer, g_optimizer, loss_stats, device,\n",
        "               epochs=150, performed_epochs=0, l1_lambda=100):\n",
        "    # Define the loss functions\n",
        "    d_real_loss = nn.BCELoss()\n",
        "    d_gan_loss = nn.BCELoss()\n",
        "    g_gan_loss = nn.BCELoss()\n",
        "    g_l1_loss = nn.L1Loss()\n",
        "\n",
        "    # Get the global paths for the generator, discriminator, and loss statistics\n",
        "    global generator_path, discriminator_path, loss_stats_path\n",
        "\n",
        "\n",
        "    ## Pre-training the generator\n",
        "    #TODO: Unclear if we should have this here or not\n",
        "    \"\"\"\n",
        "    pre_epochs = 10\n",
        "\n",
        "    for pre_epoch in range(pre_epochs):\n",
        "\n",
        "        for batch, (x, y) in tqdm(enumerate(dataloader)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            #====================#\n",
        "            # Generator training #\n",
        "            #====================#\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            z = generator(x)\n",
        "\n",
        "            # Compute the loss\n",
        "            gl1 = g_l1_loss(z, y)\n",
        "\n",
        "            g_loss = l1_lambda * gl1\n",
        "\n",
        "            # Backward pass\n",
        "            g_loss.backward()\n",
        "\n",
        "            #===================#\n",
        "            # Update weights    #\n",
        "            #===================#\n",
        "\n",
        "            # Update weights\n",
        "            g_optimizer.step()\n",
        "            print(\"This ran\")\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in tqdm(range(performed_epochs, epochs)):\n",
        "        # The loss stats of an epoch to average later\n",
        "        loss_epoch = pd.DataFrame(columns=[\"d_loss\", \"g_loss\", \"g_GAN_loss\", \"g_L1_loss\"])\n",
        "\n",
        "        for batch, (x, y) in tqdm(enumerate(dataloader)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            #========================#\n",
        "            # Discriminator training #\n",
        "            #========================#\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            z = generator(x).detach()\n",
        "            d_real = discriminator(x, y)\n",
        "            d_fake = discriminator(x, z)\n",
        "\n",
        "            # Compute the loss\n",
        "            drl = d_real_loss(d_real, th.ones_like(d_real))\n",
        "            dgl = d_gan_loss(d_fake, th.zeros_like(d_fake))\n",
        "\n",
        "            d_loss = drl + dgl\n",
        "\n",
        "            # Backward pass\n",
        "            d_loss.backward()\n",
        "\n",
        "\n",
        "            #====================#\n",
        "            # Generator training #\n",
        "            #====================#\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            z = generator(x)\n",
        "            d_fake = discriminator(x, z).detach()\n",
        "\n",
        "            # Compute the loss\n",
        "            ggl = g_gan_loss(d_fake, th.ones_like(d_fake))\n",
        "            gl1 = g_l1_loss(z, y)\n",
        "\n",
        "            g_loss = ggl + l1_lambda * gl1\n",
        "\n",
        "            # Backward pass\n",
        "            g_loss.backward()\n",
        "\n",
        "            #===================#\n",
        "            # Update weights    #\n",
        "            #===================#\n",
        "\n",
        "            # Update weights\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Update weights\n",
        "            g_optimizer.step()\n",
        "\n",
        "            #===================#\n",
        "            # Cleanup memory   #\n",
        "            #===================#\n",
        "            #TODO: Verify that this is necessary and that it works as intended\n",
        "            #del x, y, z, d_real, d_fake\n",
        "\n",
        "            # Store the batch statistics\n",
        "            loss_epoch = pd.concat([loss_epoch, pd.DataFrame({\n",
        "                \"d_loss\": d_loss.item(),\n",
        "                \"g_loss\": g_loss.item(),\n",
        "                \"g_GAN_loss\": ggl.item(),\n",
        "                \"g_L1_loss\": gl1.item()\n",
        "                }, index=[0])])\n",
        "\n",
        "\n",
        "        # Clear the output and get, print, and save the epoch average loss statistics\n",
        "        clear_output(wait=True)\n",
        "        loss_epoch_mean = {**loss_epoch.mean().to_dict(), \"Epoch\": epoch}\n",
        "        lem_df = pd.DataFrame(loss_epoch_mean, index=[0])\n",
        "        display(lem_df)\n",
        "        loss_stats = pd.concat([loss_stats, pd.DataFrame(loss_epoch_mean, index=[0])])\n",
        "\n",
        "        # Save the current generator, discriminator, and epochs\n",
        "        th.save(generator.state_dict(), generator_path)\n",
        "        th.save(discriminator.state_dict(), discriminator_path)\n",
        "        loss_stats.to_csv(loss_stats_path, index=False)\n",
        "\n",
        "        # Plot the loss statistics on different subplots\n",
        "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        axs[0, 0].plot(loss_stats[\"Epoch\"], loss_stats[\"d_loss\"])\n",
        "        axs[0, 0].set_title(\"Discriminator loss\")\n",
        "        axs[0, 1].plot(loss_stats[\"Epoch\"], loss_stats[\"g_loss\"])\n",
        "        axs[0, 1].set_title(\"Generator loss\")\n",
        "        axs[1, 0].plot(loss_stats[\"Epoch\"], loss_stats[\"g_GAN_loss\"])\n",
        "        axs[1, 0].set_title(\"Generator GAN loss\")\n",
        "        axs[1, 1].plot(loss_stats[\"Epoch\"], loss_stats[\"g_L1_loss\"])\n",
        "        axs[1, 1].set_title(\"Generator L1 loss\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return loss_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftzd-rzS4OhD"
      },
      "source": [
        "# Save/load progress\n",
        "\n",
        "To easily continue training the same model in separate runs we save all the data and can load from that state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDjlHRNe4OhD"
      },
      "source": [
        "## Dataset saving/loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YOc16Rh4OhD"
      },
      "outputs": [],
      "source": [
        "# Get the path where to save to or load from for the split indices\n",
        "split_indices_path = os.path.join(model_data, \"split_indices.pt\")\n",
        "\n",
        "# Save or load based on whether a split is already present\n",
        "if os.path.exists(split_indices_path):\n",
        "    # Load the indices\n",
        "    split_indices = th.load(split_indices_path)\n",
        "    train_indices = split_indices[\"train_indices\"]\n",
        "    validation_indices = split_indices[\"validation_indices\"]\n",
        "    test_indices = split_indices[\"test_indices\"]\n",
        "\n",
        "    # Replace the current dataloaders\n",
        "    train_set = DataLoader(dataset=Subset(dataset, train_indices),\n",
        "                           batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
        "    validation_set = DataLoader(dataset=Subset(dataset, validation_indices),\n",
        "                                batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
        "    test_set = DataLoader(dataset=Subset(dataset, test_indices),\n",
        "                          batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
        "else:\n",
        "    # Save the indices\n",
        "    th.save({\n",
        "        \"train_indices\": train_set.dataset.indices,\n",
        "        \"validation_indices\": validation_set.dataset.indices,\n",
        "        \"test_indices\": test_set.dataset.indices\n",
        "    }, split_indices_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slbrOP4p4OhE"
      },
      "source": [
        "## Model and loss statistics loading/creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmYpqjUe4OhE"
      },
      "outputs": [],
      "source": [
        "# Load or create the generator\n",
        "generator_path = os.path.join(model_data, \"generator.pth\")\n",
        "generator = Autoencoder().to(device)\n",
        "if os.path.exists(generator_path):\n",
        "    generator.load_state_dict(th.load(generator_path))\n",
        "\n",
        "# Load or create the discriminator\n",
        "discriminator_path = os.path.join(model_data, \"discriminator.pth\")\n",
        "discriminator = Discriminator().to(device)\n",
        "if os.path.exists(discriminator_path):\n",
        "    discriminator.load_state_dict(th.load(discriminator_path))\n",
        "\n",
        "# The loss stats to keep track of\n",
        "loss_stats_path = os.path.join(model_data, \"loss_stats.csv\")\n",
        "performed_epochs = 0\n",
        "if os.path.exists(loss_stats_path):\n",
        "    loss_stats = pd.read_csv(loss_stats_path)\n",
        "    performed_epochs = loss_stats.shape[0]\n",
        "else:\n",
        "    loss_stats = pd.DataFrame(columns=[\"Epoch\",\"d_loss\", \"g_loss\", \"g_GAN_loss\", \"g_L1_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aLy_w3X4OhE"
      },
      "source": [
        "## Running the loop\n",
        "\n",
        "Description of various GAN training problems:\n",
        "- https://developers.google.com/machine-learning/gan/problems\n",
        "- https://arxiv.org/pdf/2005.00065.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rqdNIrd84OhE",
        "outputId": "78ec1f6a-1532-41bb-b8a8-7bb9081bbdad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>d_loss</th>\n",
              "      <th>g_loss</th>\n",
              "      <th>g_GAN_loss</th>\n",
              "      <th>g_L1_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.690900</td>\n",
              "      <td>25.796332</td>\n",
              "      <td>2.156586</td>\n",
              "      <td>0.236397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.081620</td>\n",
              "      <td>20.561441</td>\n",
              "      <td>4.165437</td>\n",
              "      <td>0.163960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.072849</td>\n",
              "      <td>20.584204</td>\n",
              "      <td>5.062907</td>\n",
              "      <td>0.155213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.146687</td>\n",
              "      <td>19.666224</td>\n",
              "      <td>4.977840</td>\n",
              "      <td>0.146884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.179313</td>\n",
              "      <td>18.983217</td>\n",
              "      <td>4.798198</td>\n",
              "      <td>0.141850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>145</td>\n",
              "      <td>0.009170</td>\n",
              "      <td>17.953102</td>\n",
              "      <td>8.656058</td>\n",
              "      <td>0.092970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>146</td>\n",
              "      <td>0.059597</td>\n",
              "      <td>19.026347</td>\n",
              "      <td>9.106178</td>\n",
              "      <td>0.099202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>147</td>\n",
              "      <td>0.198651</td>\n",
              "      <td>15.148454</td>\n",
              "      <td>5.767805</td>\n",
              "      <td>0.093806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>148</td>\n",
              "      <td>0.021883</td>\n",
              "      <td>17.442528</td>\n",
              "      <td>7.978563</td>\n",
              "      <td>0.094640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>149</td>\n",
              "      <td>0.010861</td>\n",
              "      <td>17.358373</td>\n",
              "      <td>7.924519</td>\n",
              "      <td>0.094339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Epoch    d_loss     g_loss  g_GAN_loss  g_L1_loss\n",
              "0        0  0.690900  25.796332    2.156586   0.236397\n",
              "1        1  0.081620  20.561441    4.165437   0.163960\n",
              "2        2  0.072849  20.584204    5.062907   0.155213\n",
              "3        3  0.146687  19.666224    4.977840   0.146884\n",
              "4        4  0.179313  18.983217    4.798198   0.141850\n",
              "..     ...       ...        ...         ...        ...\n",
              "145    145  0.009170  17.953102    8.656058   0.092970\n",
              "146    146  0.059597  19.026347    9.106178   0.099202\n",
              "147    147  0.198651  15.148454    5.767805   0.093806\n",
              "148    148  0.021883  17.442528    7.978563   0.094640\n",
              "149    149  0.010861  17.358373    7.924519   0.094339\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "d_optimizer = optim.Adam(\n",
        "    discriminator.parameters(),\n",
        "    lr=hyperparams[\"learning_rate\"],\n",
        "    betas=hyperparams[\"adam_betas\"])\n",
        "\n",
        "g_optimizer = optim.Adam(\n",
        "    generator.parameters(),\n",
        "    lr=hyperparams[\"learning_rate\"],\n",
        "    betas=hyperparams[\"adam_betas\"])\n",
        "\n",
        "loss_stats = train_loop(\n",
        "    train_set,\n",
        "    discriminator,\n",
        "    generator,\n",
        "    d_optimizer,\n",
        "    g_optimizer,\n",
        "    loss_stats,\n",
        "    device,\n",
        "    epochs=hyperparams[\"epochs\"],\n",
        "    performed_epochs=performed_epochs,\n",
        "    l1_lambda=hyperparams[\"l1_lambda\"])\n",
        "\n",
        "display(loss_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx08DgX34OhF"
      },
      "source": [
        "### Plotting traing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rG6AmOJP4OhF"
      },
      "outputs": [],
      "source": [
        "# Plot the loss statistics on different subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axs[0, 0].plot(loss_stats[\"Epoch\"], loss_stats[\"d_loss\"])\n",
        "axs[0, 0].set_title(\"Discriminator loss\")\n",
        "axs[0, 1].plot(loss_stats[\"Epoch\"], loss_stats[\"g_loss\"])\n",
        "axs[0, 1].set_title(\"Generator loss\")\n",
        "axs[1, 0].plot(loss_stats[\"Epoch\"], loss_stats[\"g_GAN_loss\"])\n",
        "axs[1, 0].set_title(\"Generator GAN loss\")\n",
        "axs[1, 1].plot(loss_stats[\"Epoch\"], loss_stats[\"g_L1_loss\"])\n",
        "axs[1, 1].set_title(\"Generator L1 loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrXVJP-Z4OhG"
      },
      "source": [
        "### Testing image generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "euu94C0p4OhG"
      },
      "outputs": [],
      "source": [
        "# Disable gradient computation\n",
        "with th.no_grad():\n",
        "    # Get dataset images\n",
        "    batch_input, batch_gt = next(iter(train_set))\n",
        "\n",
        "    # Generate a batch of enhanced images\n",
        "    enhanced_images = generator(batch_input.to(device)).cpu()\n",
        "\n",
        "    # Print the input, output, and groud truth for all images in the test batch\n",
        "    for i in range(len(enhanced_images)):\n",
        "        input_img = batch_input[i]\n",
        "        output_img = enhanced_images[i]\n",
        "        gt_image = batch_gt[i]\n",
        "\n",
        "        # If the images are normalized to range [-1, 1], adjust the values for visualization\n",
        "        input_img = (input_img + 1) / 2\n",
        "        output_img = (output_img + 1) / 2\n",
        "        gt_image = (gt_image + 1) / 2\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(input_img.permute(1, 2, 0))\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(output_img.permute(1, 2, 0))\n",
        "        plt.title(\"Output Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(gt_image.permute(1, 2, 0))\n",
        "        plt.title(\"Ground Truth Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMdmNFSY4OhG"
      },
      "source": [
        "### Testing the predefined 5 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "id": "3auqCaqs4OhH"
      },
      "outputs": [],
      "source": [
        "# Disable gradient computation\n",
        "with th.no_grad():\n",
        "    # Get a batch (all) of the predefined images\n",
        "    batch_input, batch_gt = next(iter(figure_set))\n",
        "\n",
        "    # Generate the enhanced images and show them\n",
        "    enhanced_images = generator(batch_input.to(device)).cpu()\n",
        "    for i in range(len(enhanced_images)):\n",
        "        input_img = batch_input[i]\n",
        "        output_img = enhanced_images[i]\n",
        "        gt_image = batch_gt[i]\n",
        "\n",
        "        # If the images are normalized to range [-1, 1], adjust the values for visualization\n",
        "        input_img = (input_img + 1) / 2\n",
        "        output_img = (output_img + 1) / 2\n",
        "        gt_image = (gt_image + 1) / 2\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(input_img.permute(1, 2, 0))\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(output_img.permute(1, 2, 0))\n",
        "        plt.title(\"Output Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(gt_image.permute(1, 2, 0))\n",
        "        plt.title(\"Ground Truth Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ff9cTbH4OhH"
      },
      "source": [
        "# Model evaluation\n",
        "\n",
        "## Setting model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kl3EFysa4OhH"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "generator.eval()\n",
        "discriminator.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x2bb6024OhI"
      },
      "source": [
        "## Defining the metrics\n",
        "\n",
        "All metrics as implemented support multibatch processing. That is to say they support tensor input on the form:\n",
        "\n",
        "[n, c=3, w, h] where n i the number of samples per batch. c is the number of colour channels. w and h are width/height respectively.\n",
        "\n",
        "This may make code readability somewhat reduced.\n",
        "\n",
        "### The UIQM rabbithole\n",
        "\n",
        "UIQM is surprisingly complex. The different submetrics are appropriately broken down into separate functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDGlsTfn4OhI"
      },
      "outputs": [],
      "source": [
        "def PSNR(x, y):\n",
        "    \"\"\"Peak Signal-to-Noise Ratio (PSNR)\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor, generated\n",
        "        y (np.ndarray): Image tensor, ground truth\n",
        "\n",
        "    Returns:\n",
        "        float: PSNR value\n",
        "    \"\"\"\n",
        "    # Maximum possible pixel value\n",
        "    MAX = 1.0\n",
        "\n",
        "    n, c, w, h = x.shape\n",
        "\n",
        "    # Flattening each image, while retaining batch axis\n",
        "    x = np.reshape(x, (n, c * w * h))\n",
        "    y = np.reshape(y, (n, c * w * h))\n",
        "\n",
        "    # Take the mean of the x-y difference along the batch axis\n",
        "    mean = np.mean((x - y) ** 2, axis=1)\n",
        "\n",
        "    # Compute the PSNR\n",
        "    psnr = 10 * np.log10(MAX / mean)\n",
        "\n",
        "    return psnr\n",
        "\n",
        "def SSIM(x, y):\n",
        "    \"\"\"Structural Similarity Index Measure (SSIM)\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor, generated\n",
        "        y (np.ndarray): Image tensor, ground truth\n",
        "\n",
        "    Returns:\n",
        "        float: SSIM value\n",
        "    \"\"\"\n",
        "    # Maximum possible pixel value\n",
        "    L = 1.0\n",
        "\n",
        "    # Constants\n",
        "    C1 = (0.01 * L) ** 2\n",
        "    C2 = (0.03 * L) ** 2\n",
        "\n",
        "    # Compute the mean of x and y along the channel, width, and height axes\n",
        "    mu_x = np.mean(x, axis=(1,2,3))\n",
        "    mu_y = np.mean(y, axis=(1,2,3))\n",
        "\n",
        "    # Create a n, c, w, h tensor of the mean of x and y\n",
        "    mu_x_b = np.broadcast_to(mu_x[:, np.newaxis, np.newaxis, np.newaxis], x.shape)\n",
        "    mu_y_b = np.broadcast_to(mu_y[:, np.newaxis, np.newaxis, np.newaxis], y.shape)\n",
        "\n",
        "    # Compute the variance and covariance of x and y\n",
        "    sigma_x = np.mean((x - mu_x_b) ** 2, axis=(1, 2, 3))\n",
        "    sigma_y = np.mean((y - mu_y_b) ** 2, axis=(1, 2, 3))\n",
        "\n",
        "    sigma_xy = np.mean((x - mu_x_b) * (y - mu_y_b), axis=(1, 2, 3))\n",
        "\n",
        "    # Compute the numerator and denominator of the SSIM\n",
        "    num = (2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)\n",
        "    den = (mu_x ** 2 + mu_y ** 2 + C1) * (sigma_x + sigma_y + C2)\n",
        "\n",
        "    # Take the elementwise ratio of the numerator and denominator to obtain the SSIM\n",
        "    ssim = num / den\n",
        "\n",
        "    return ssim\n",
        "\n",
        "def UIQM(x):\n",
        "    \"\"\"Underwater Image Quality Measure (UIQM)\n",
        "    NOTE: Works for multibatch inputs [n, 3, w, h]\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor\n",
        "\n",
        "    Returns:\n",
        "        float: UIQM value\n",
        "    \"\"\"\n",
        "\n",
        "    # Constants, as per the Sea-Pix-GAN paper\n",
        "    c1 = 0.0282\n",
        "    c2 = 0.2953\n",
        "    c3 = 3.5753\n",
        "\n",
        "    # Compute the UICM, UISM, and UIConM\n",
        "    uicm = UICM(x)\n",
        "    uism = UISM(x)\n",
        "    uiconm = UIConM(x)\n",
        "\n",
        "    return c1 * uicm + c2 * uism + c3 * uiconm\n",
        "\n",
        "def UICM(x):\n",
        "    \"\"\"Underwater Image Colorfulness Measure (UICM)\n",
        "    NOTE: Works for multibatch inputs [n, 3, w, h]\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor\n",
        "\n",
        "    Returns:\n",
        "        float: UICM value\n",
        "    \"\"\"\n",
        "    # Extract the R, G, and B channels of all images in batch\n",
        "    R = x[:,0, :, :]\n",
        "    G = x[:,1, :, :]\n",
        "    B = x[:,2, :, :]\n",
        "\n",
        "    # Flatten the channels for batch\n",
        "    n, w, h = R.shape\n",
        "    R = np.reshape(R, (n, w * h))\n",
        "    G = np.reshape(G, (n, w * h))\n",
        "    B = np.reshape(B, (n, w * h))\n",
        "\n",
        "    # Set alpha\n",
        "    alpha = 0.1\n",
        "\n",
        "    # Create the RG and YB channels\n",
        "    RG = R - G\n",
        "    YB = 0.5 * (R + G) - B\n",
        "\n",
        "    # Compute the alpha trimmed distribution along each image\n",
        "    aRG = stats.trimboth(RG, alpha, axis=1)\n",
        "    aYB = stats.trimboth(YB, alpha, axis=1)\n",
        "\n",
        "    # Compute the mean along each image\n",
        "    mean_aRG = np.mean(aRG, axis=1)\n",
        "    mean_aYB = np.mean(aYB, axis=1)\n",
        "\n",
        "    # Compute the alpha trimmed standard deviation along each image\n",
        "    std_aRG = np.sum((RG - mean_aRG[:, np.newaxis]) ** 2, axis=1) / (w * h - 1)\n",
        "    std_aYB = np.sum((YB - mean_aYB[:, np.newaxis]) ** 2, axis=1) / (w * h - 1)\n",
        "\n",
        "    std_aRG = np.sqrt(std_aRG)\n",
        "    std_aYB = np.sqrt(std_aYB)\n",
        "\n",
        "    # Constants, as per HVSIUIQM paper\n",
        "    c1 = -0.0282\n",
        "    c2 = 0.1586\n",
        "\n",
        "    # Compute the 2 terms of the UICM\n",
        "    term_1 = np.sqrt(mean_aRG ** 2 + mean_aYB ** 2)\n",
        "    term_2 = np.sqrt(std_aRG ** 2 + std_aYB ** 2) #TODO: can std be converted to variance to save computation?\n",
        "\n",
        "    return c1 * term_1 + c2 * term_2\n",
        "\n",
        "def UISM(x):\n",
        "    \"\"\"Underwater Image Sharpness Measure (UISM)\n",
        "    NOTE: Works for multibatch inputs [n, 3, w, h]\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor\n",
        "\n",
        "    Returns:\n",
        "        float: UISM value\n",
        "    \"\"\"\n",
        "    #=======================#\n",
        "    # Edge detection filter #\n",
        "    #=======================#\n",
        "\n",
        "    # Run the image through a Sobel filter, along the x and y axes respectively.\n",
        "    x_grad = ndimage.sobel(x, axis=0)\n",
        "    y_grad = ndimage.sobel(x, axis=1)\n",
        "\n",
        "    # Then compute the magnitude of the gradients (hypotenuse of the two gradients)\n",
        "    filtered_x = np.hypot(x_grad, y_grad)\n",
        "\n",
        "    #=========================#\n",
        "    # Enhancement measurement #\n",
        "    #=========================#\n",
        "\n",
        "    # Compute Enhancement Measure Estimation (EME) for each channel using the filtered image\n",
        "    EME_R = EME(filtered_x[:,0, :, :])\n",
        "    EME_G = EME(filtered_x[:,1, :, :])\n",
        "    EME_B = EME(filtered_x[:,2, :, :])\n",
        "\n",
        "    # Compute the weighted EME, using standard RGB channel weights\n",
        "    lambda_R = 0.299\n",
        "    lambda_G = 0.587\n",
        "    lambda_B = 0.114\n",
        "\n",
        "    return lambda_R * EME_R + lambda_G * EME_G + lambda_B * EME_B\n",
        "\n",
        "def EME(x):\n",
        "    \"\"\"Enhancement Measure Estimation (EME)\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor, sobel filtered\n",
        "\n",
        "    Returns:\n",
        "        float: EME value\n",
        "    \"\"\"\n",
        "    #TODO: What should the block size be? Unclear given paper (the one with the looong name)\n",
        "    # Set the k1 and k2 constants as width and height of the image. n is the number of images in the batch\n",
        "    n, k1, k2 = x.shape\n",
        "\n",
        "    norm_const = 2 / (k1 * k2)\n",
        "\n",
        "    EME = 0\n",
        "\n",
        "    for i in range(k1):\n",
        "        for j in range(k2):\n",
        "\n",
        "            # Find max and min values in the block\n",
        "            max_val = np.max(x[:, i:i+k1, j:j+k2])\n",
        "            min_val = np.min(x[:, i:i+k1, j:j+k2]) + 1e-4\n",
        "\n",
        "            # Define a small epsilon to avoid division by zero\n",
        "            eps = 1e-3\n",
        "\n",
        "            # Compute the local contrast\n",
        "            local_contrast = np.log(max_val / (min_val + eps))\n",
        "\n",
        "            EME += local_contrast\n",
        "\n",
        "    return norm_const * EME\n",
        "\n",
        "def UIConM(x):\n",
        "    \"\"\"Underwater Image Contrast Measure (UIConM)\n",
        "    NOTE: Works for multibatch inputs [n, 3, w, h]\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor\n",
        "\n",
        "    Returns:\n",
        "        float: UIConM value\n",
        "    \"\"\"\n",
        "    # RGB channel weights\n",
        "    lambda_R = 0.299\n",
        "    lambda_G = 0.587\n",
        "    lambda_B = 0.114\n",
        "\n",
        "    # Exract the intensity of the image, using standard RGB channel weights\n",
        "    I = lambda_R * x[:,0, :, :] + lambda_G * x[:,1, :, :] + lambda_B * x[:,2, :, :]\n",
        "\n",
        "    # Compute the logAMEE(Intensity)\n",
        "    logamee = logAMEE(I)\n",
        "\n",
        "    return logamee\n",
        "\n",
        "def logAMEE(x):\n",
        "    \"\"\"Logarithmic Agaian Measure of Enhancement by Engropy (logAMEE)\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Image tensor, weighted intensity\n",
        "\n",
        "    Returns:\n",
        "        float: logAMEE value\n",
        "    \"\"\"\n",
        "    # Define the PLIP operators\n",
        "    add = lambda x,y: x + y - x * y         # circle-plus operator\n",
        "    mul = lambda x,y: 1 - (1 - y)**x        # circle-multiply operator\n",
        "    diff = lambda x,y: (x - y) / (1 - y)    # Theta operator\n",
        "\n",
        "    # Define k1 and k2 as the width and height of the image\n",
        "    n, k1, k2 = x.shape #TODO: Once again, what should the block size actually be? Check the paper again.\n",
        "\n",
        "\n",
        "    # Define the constants\n",
        "    norm_const = 1 / (k1 * k2)\n",
        "\n",
        "    logamee = 0\n",
        "\n",
        "    for i in range(k1):\n",
        "        for j in range(k2):\n",
        "\n",
        "            # Extract the block\n",
        "            block = x[:, i:i+k1, j:j+k2]\n",
        "\n",
        "            # Find the maximum and minimum values in the block\n",
        "            max_val = np.max(block)\n",
        "            min_val = np.min(block)\n",
        "\n",
        "            I_diff = diff(max_val, min_val)\n",
        "            I_add = add(max_val, min_val)\n",
        "\n",
        "            ratio = I_diff / I_add\n",
        "\n",
        "            logamee += ratio * np.log(ratio)\n",
        "\n",
        "    return mul(norm_const, logamee)\n",
        "\n",
        "def get_metrics(test_set, generator, DEBUG=False):\n",
        "    # Set the frame size according to the debug mode\n",
        "    frame_size = 500 if not DEBUG else 64\n",
        "\n",
        "    # Initialize the metrics dataframe\n",
        "    metrics = pd.DataFrame(columns=[\"PSNR\", \"SSIM\", \"UIQM (Generated)\", \"UIQM (Ground Truth)\"])\n",
        "\n",
        "    # While the length of metrics is < 500, keep sampling\n",
        "    # batches from the test data and computing the metrics\n",
        "    while len(metrics) < frame_size:\n",
        "\n",
        "        print(f\"Metric samples computed: {len(metrics)}\")\n",
        "\n",
        "        # Disable gradient computation\n",
        "        with th.no_grad():\n",
        "\n",
        "            x, y = next(iter(test_set))\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.detach().cpu().numpy()\n",
        "\n",
        "            enhanced = generator(x).detach().cpu().numpy()\n",
        "\n",
        "            # Normalize all images to [0, 1]\n",
        "            x = (x + 1) / 2\n",
        "            enhanced = (enhanced + 1) / 2\n",
        "            y = (y + 1) / 2\n",
        "\n",
        "            psnr = PSNR(y, enhanced)\n",
        "            ssim = SSIM(y, enhanced)\n",
        "            uiqm = UIQM(enhanced)\n",
        "            uiqm_gt = UIQM(y)\n",
        "\n",
        "            batch_metrics = pd.DataFrame({\n",
        "                \"PSNR\": psnr,\n",
        "                \"SSIM\": ssim,\n",
        "                \"UIQM (Generated)\": uiqm,\n",
        "                \"UIQM (Ground Truth)\": uiqm_gt\n",
        "                })\n",
        "\n",
        "            metrics = pd.concat([metrics, batch_metrics])\n",
        "\n",
        "    # Discard any rows beyond the 500th\n",
        "    metrics = metrics.head(frame_size)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7i3LUGC4OhJ"
      },
      "source": [
        "## Getting the test metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "id": "V-tBl-ie4OhJ"
      },
      "outputs": [],
      "source": [
        "raw_metrics = get_metrics(test_set, generator, DEBUG=False)\n",
        "summary = pd.DataFrame(columns=[\"Mean\", \"Std\"])\n",
        "\n",
        "summary.loc[\"PSNR\"] = [raw_metrics[\"PSNR\"].mean(), raw_metrics[\"PSNR\"].std()]\n",
        "summary.loc[\"SSIM\"] = [raw_metrics[\"SSIM\"].mean(), raw_metrics[\"SSIM\"].std()]\n",
        "summary.loc[\"UIQM (Generated)\"] = [raw_metrics[\"UIQM (Generated)\"].mean(), raw_metrics[\"UIQM (Generated)\"].std()]\n",
        "summary.loc[\"UIQM (Ground Truth)\"] = [raw_metrics[\"UIQM (Ground Truth)\"].mean(), raw_metrics[\"UIQM (Ground Truth)\"].std()]\n",
        "\n",
        "display(raw_metrics.head())\n",
        "display(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2Pf9qK4OhK"
      },
      "source": [
        "### Saving the test metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O38lECs-4OhK"
      },
      "outputs": [],
      "source": [
        "raw_metrics.to_csv(os.path.join(model_data, \"raw_metrics.csv\"), index=False)\n",
        "summary.to_csv(os.path.join(model_data, \"summary.csv\"), index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
